{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This document explores using tensorflow (and maybe keras) on the MNIST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'win32'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'darwin' in sys.platform:\n",
    "    data_dir = os.path.join('/Users', 'njchiang', 'CloudStation',\n",
    "                            'kaggle', 'digits')\n",
    "else:\n",
    "    data_dir = os.path.join('D:\\\\', 'CloudStation', 'kaggle', 'digits')\n",
    "    \n",
    "data = np.loadtxt(os.path.join(data_dir, 'train.csv'), delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784) (42000, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train_full = data[:, 0:1]\n",
    "x_train_full = data[:, 1:]\n",
    "print(x_train_full.shape, y_train_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_full, y_train_full, test_size=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train_vec = to_categorical(y_train)\n",
    "y_test_vec = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, x_train_full.shape[1]])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, y_train_vec.shape[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are defined by their weights and biases, and we can specify the neuronal function at each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some layer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def softmax_dense(x, W, b):\n",
    "    return tf.matmul(x, W) + b\n",
    "\n",
    "\n",
    "def relu_dense(x, W, b):\n",
    "    return tf.nn.relu(softmax_dense(x, W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a dense (only) network with some dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = [x_train.shape[1], 100, y_train_vec.shape[1]]\n",
    "# layer 1\n",
    "h_fc1 = relu_dense(x, weight_variable([architecture[0], architecture[1]]), \n",
    "                   bias_variable([architecture[1]]))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# layer 2\n",
    "y_res = softmax_dense(h_fc1_drop, weight_variable([architecture[1], architecture[2]]), \n",
    "                   bias_variable([architecture[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_res))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_res,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write my own batching function:\n",
    "def get_next_batch(n, x, y):\n",
    "    batch_idx = np.random.choice(range(x.shape[0]), n)\n",
    "    return [x[batch_idx], y[batch_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08\nstep 1, training accuracy 0.08\nstep 2, training accuracy 0.08\nstep 3, training accuracy 0.08\nstep 4, training accuracy 0.08\nstep 5, training accuracy 0.08\nstep 6, training accuracy 0.08\nstep 7, training accuracy 0.08\nstep 8, training accuracy 0.08\nstep 9, training accuracy 0.08\nstep 10, training accuracy 0.08\nstep 11, training accuracy 0.08\nstep 12, training accuracy 0.08\nstep 13, training accuracy 0.08\nstep 14, training accuracy 0.08\nstep 15, training accuracy 0.08\nstep 16, training accuracy 0.08\nstep 17, training accuracy 0.08\nstep 18, training accuracy 0.08\nstep 19, training accuracy 0.08\nstep 20, training accuracy 0.08\nstep 21, training accuracy 0.08\nstep 22, training accuracy 0.08\nstep 23, training accuracy 0.08\nstep 24, training accuracy 0.08\nstep 25, training accuracy 0.08\nstep 26, training accuracy 0.08\nstep 27, training accuracy 0.08\nstep 28, training accuracy 0.08\nstep 29, training accuracy 0.08\nstep 30, training accuracy 0.08\nstep 31, training accuracy 0.08\nstep 32, training accuracy 0.08\nstep 33, training accuracy 0.08\nstep 34, training accuracy 0.08\nstep 35, training accuracy 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 36, training accuracy 0.08\nstep 37, training accuracy 0.08\nstep 38, training accuracy 0.08\nstep 39, training accuracy 0.08\nstep 40, training accuracy 0.08\nstep 41, training accuracy 0.08\nstep 42, training accuracy 0.08\nstep 43, training accuracy 0.08\nstep 44, training accuracy 0.08\nstep 45, training accuracy 0.08\nstep 46, training accuracy 0.08\nstep 47, training accuracy 0.08\nstep 48, training accuracy 0.08\nstep 49, training accuracy 0.08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nstep 50, training accuracy 0.08\nstep 51, training accuracy 0.08\nstep 52, training accuracy 0.08\nstep 53, training accuracy 0.08\nstep 54, training accuracy 0.08\nstep 55, training accuracy 0.08\nstep 56, training accuracy 0.08\nstep 57, training accuracy 0.08\nstep 58, training accuracy 0.08\nstep 59, training accuracy 0.08\nstep 60, training accuracy 0.08\nstep 61, training accuracy 0.08\nstep 62, training accuracy 0.08\nstep 63, training accuracy 0.08\nstep 64, training accuracy 0.08\nstep 65, training accuracy 0.08\nstep 66, training accuracy 0.08\nstep 67, training accuracy 0.08\nstep 68, training accuracy 0.08\nstep 69, training accuracy 0.08\nstep 70, training accuracy 0.08\nstep 71, training accuracy 0.08\nstep 72, training accuracy 0.08\nstep 73, training accuracy 0.08\nstep 74, training accuracy 0.08\nstep 75, training accuracy 0.08\nstep 76, training accuracy 0.08\nstep 77, training accuracy 0.08\nstep 78, training accuracy 0.08\nstep 79, training accuracy 0.08\nstep 80, training accuracy 0.08\nstep 81, training accuracy 0.08\nstep 82, training accuracy 0.08\nstep 83, training accuracy 0.08\nstep 84, training accuracy 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 85, training accuracy 0.08\nstep 86, training accuracy 0.08\nstep 87, training accuracy 0.08\nstep 88, training accuracy 0.08\nstep 89, training accuracy 0.08\nstep 90, training accuracy 0.08\nstep 91, training accuracy 0.08\nstep 92, training accuracy 0.08\nstep 93, training accuracy 0.08\nstep 94, training accuracy 0.08\nstep 95, training accuracy 0.08\nstep 96, training accuracy 0.08\nstep 97, training accuracy 0.08\nstep 98, training accuracy 0.08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nstep 99, training accuracy 0.08\nstep 100, training accuracy 0.28\nstep 101, training accuracy 0.28\nstep 102, training accuracy 0.28\nstep 103, training accuracy 0.28\nstep 104, training accuracy 0.28\nstep 105, training accuracy 0.28\nstep 106, training accuracy 0.28\nstep 107, training accuracy 0.28\nstep 108, training accuracy 0.28\nstep 109, training accuracy 0.28\nstep 110, training accuracy 0.28\nstep 111, training accuracy 0.28\nstep 112, training accuracy 0.28\nstep 113, training accuracy 0.28\nstep 114, training accuracy 0.28\nstep 115, training accuracy 0.28\nstep 116, training accuracy 0.28\nstep 117, training accuracy 0.28\nstep 118, training accuracy 0.28\nstep 119, training accuracy 0.28\nstep 120, training accuracy 0.28\nstep 121, training accuracy 0.28\nstep 122, training accuracy 0.28\nstep 123, training accuracy 0.28\nstep 124, training accuracy 0.28\nstep 125, training accuracy 0.28\nstep 126, training accuracy 0.28\nstep 127, training accuracy 0.28\nstep 128, training accuracy 0.28\nstep 129, training accuracy 0.28\nstep 130, training accuracy 0.28\nstep 131, training accuracy 0.28\nstep 132, training accuracy 0.28\nstep 133, training accuracy 0.28"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nstep 134, training accuracy 0.28\nstep 135, training accuracy 0.28\nstep 136, training accuracy 0.28\nstep 137, training accuracy 0.28\nstep 138, training accuracy 0.28\nstep 139, training accuracy 0.28\nstep 140, training accuracy 0.28\nstep 141, training accuracy 0.28\nstep 142, training accuracy 0.28\nstep 143, training accuracy 0.28\nstep 144, training accuracy 0.28\nstep 145, training accuracy 0.28\nstep 146, training accuracy 0.28"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nstep 147, training accuracy 0.28\nstep 148, training accuracy 0.28\nstep 149, training accuracy 0.28\nstep 150, training accuracy 0.28\nstep 151, training accuracy 0.28\nstep 152, training accuracy 0.28\nstep 153, training accuracy 0.28\nstep 154, training accuracy 0.28\nstep 155, training accuracy 0.28\nstep 156, training accuracy 0.28\nstep 157, training accuracy 0.28\nstep 158, training accuracy 0.28\nstep 159, training accuracy 0.28\nstep 160, training accuracy 0.28\nstep 161, training accuracy 0.28\nstep 162, training accuracy 0.28\nstep 163, training accuracy 0.28\nstep 164, training accuracy 0.28\nstep 165, training accuracy 0.28\nstep 166, training accuracy 0.28\nstep 167, training accuracy 0.28\nstep 168, training accuracy 0.28\nstep 169, training accuracy 0.28\nstep 170, training accuracy 0.28\nstep 171, training accuracy 0.28\nstep 172, training accuracy 0.28\nstep 173, training accuracy 0.28\nstep 174, training accuracy 0.28\nstep 175, training accuracy 0.28\nstep 176, training accuracy 0.28\nstep 177, training accuracy 0.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 178, training accuracy 0.28\nstep 179, training accuracy 0.28\nstep 180, training accuracy 0.28\nstep 181, training accuracy 0.28\nstep 182, training accuracy 0.28\nstep 183, training accuracy 0.28\nstep 184, training accuracy 0.28\nstep 185, training accuracy 0.28\nstep 186, training accuracy 0.28\nstep 187, training accuracy 0.28\nstep 188, training accuracy 0.28\nstep 189, training accuracy 0.28\nstep 190, training accuracy 0.28\nstep 191, training accuracy 0.28\nstep 192, training accuracy 0.28\nstep 193, training accuracy 0.28"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nstep 194, training accuracy 0.28\nstep 195, training accuracy 0.28\nstep 196, training accuracy 0.28\nstep 197, training accuracy 0.28\nstep 198, training accuracy 0.28\nstep 199, training accuracy 0.28\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(200):\n",
    "    batch = get_next_batch(50, x_train, y_train_vec)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.474345\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: x_test, y_: y_test_vec, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_relu_softmax_size = [x_train.shape[1], 32, 100, y_train_vec.shape[1]]  # fully connected architecture\n",
    "\n",
    "# reshape as image\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "# convolution and max pooling (layer 1)\n",
    "W_conv1 = weight_variable([5, 5, 1, conv_relu_softmax_size[1]])\n",
    "b_conv1 = bias_variable([conv_relu_softmax_size[1]])\n",
    "\n",
    "h_conv1 = conv2d(x_image, W_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "h_pool1_vec = tf.reshape(h_pool1, [-1, 14*14*conv_relu_softmax_size[1]])\n",
    "\n",
    "# layer 2\n",
    "h_fc1 = relu_dense(h_pool1_vec, weight_variable([14*14*conv_relu_softmax_size[1], \n",
    "                                                 conv_relu_softmax_size[2]]), \n",
    "                   bias_variable([conv_relu_softmax_size[2]]))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "# \n",
    "# layer 3\n",
    "y_res = softmax_dense(h_fc1_drop, weight_variable([conv_relu_softmax_size[2], \n",
    "                                                   conv_relu_softmax_size[3]]), \n",
    "                      bias_variable([conv_relu_softmax_size[3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_res))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_res,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.1\nstep 1, training accuracy 0.1\nstep 2, training accuracy 0.1\nstep 3, training accuracy 0.1\nstep 4, training accuracy 0.1\nstep 5, training accuracy 0.1\nstep 6, training accuracy 0.1\nstep 7, training accuracy 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8, training accuracy 0.1\nstep 9, training accuracy 0.1\nstep 10, training accuracy 0.1\nstep 11, training accuracy 0.1\nstep 12, training accuracy 0.1\nstep 13, training accuracy 0.1\nstep 14, training accuracy 0.1\nstep 15, training accuracy 0.1\nstep 16, training accuracy 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17, training accuracy 0.1\nstep 18, training accuracy 0.1\nstep 19, training accuracy 0.1\nstep 20, training accuracy 0.1\nstep 21, training accuracy 0.1\nstep 22, training accuracy 0.1\nstep 23, training accuracy 0.1\nstep 24, training accuracy 0.1\nstep 25, training accuracy 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 26, training accuracy 0.1\nstep 27, training accuracy 0.1\nstep 28, training accuracy 0.1\nstep 29, training accuracy 0.1\nstep 30, training accuracy 0.1\nstep 31, training accuracy 0.1\nstep 32, training accuracy 0.1\nstep 33, training accuracy 0.1\nstep 34, training accuracy 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 35, training accuracy 0.1\nstep 36, training accuracy 0.1\nstep 37, training accuracy 0.1\nstep 38, training accuracy 0.1\nstep 39, training accuracy 0.1\nstep 40, training accuracy 0.1\nstep 41, training accuracy 0.1\nstep 42, training accuracy 0.1\nstep 43, training accuracy 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 44, training accuracy 0.1\nstep 45, training accuracy 0.1\nstep 46, training accuracy 0.1\nstep 47, training accuracy 0.1\nstep 48, training accuracy 0.1\nstep 49, training accuracy 0.1\nstep 50, training accuracy 0.1\nstep 51, training accuracy 0.1\nstep 52, training accuracy 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 53, training accuracy 0.1\nstep 54, training accuracy 0.1\nstep 55, training accuracy 0.1\nstep 56, training accuracy 0.1\nstep 57, training accuracy 0.1\nstep 58, training accuracy 0.1\nstep 59, training accuracy 0.1\nstep 60, training accuracy 0.1\nstep 61, training accuracy 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 62, training accuracy 0.1\nstep 63, training accuracy 0.1\nstep 64, training accuracy 0.1\nstep 65, training accuracy 0.1\nstep 66, training accuracy 0.1\nstep 67, training accuracy 0.1\nstep 68, training accuracy 0.1\nstep 69, training accuracy 0.1\nstep 70, training accuracy 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 71, training accuracy 0.1\nstep 72, training accuracy 0.1\nstep 73, training accuracy 0.1\nstep 74, training accuracy 0.1\nstep 75, training accuracy 0.1\nstep 76, training accuracy 0.1\nstep 77, training accuracy 0.1\nstep 78, training accuracy 0.1\nstep 79, training accuracy 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 80, training accuracy 0.1\nstep 81, training accuracy 0.1\nstep 82, training accuracy 0.1\nstep 83, training accuracy 0.1\nstep 84, training accuracy 0.1\nstep 85, training accuracy 0.1\nstep 86, training accuracy 0.1\nstep 87, training accuracy 0.1\nstep 88, training accuracy 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 89, training accuracy 0.1\nstep 90, training accuracy 0.1\nstep 91, training accuracy 0.1\nstep 92, training accuracy 0.1\nstep 93, training accuracy 0.1\nstep 94, training accuracy 0.1\nstep 95, training accuracy 0.1\nstep 96, training accuracy 0.1\nstep 97, training accuracy 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 98, training accuracy 0.1\nstep 99, training accuracy 0.1\nstep 100, training accuracy 0.58\nstep 101, training accuracy 0.58\nstep 102, training accuracy 0.58\nstep 103, training accuracy 0.58\nstep 104, training accuracy 0.58\nstep 105, training accuracy 0.58\nstep 106, training accuracy 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 107, training accuracy 0.58\nstep 108, training accuracy 0.58\nstep 109, training accuracy 0.58\nstep 110, training accuracy 0.58\nstep 111, training accuracy 0.58\nstep 112, training accuracy 0.58\nstep 113, training accuracy 0.58\nstep 114, training accuracy 0.58\nstep 115, training accuracy 0.58\nstep 116, training accuracy 0.58"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nstep 117, training accuracy 0.58\nstep 118, training accuracy 0.58\nstep 119, training accuracy 0.58\nstep 120, training accuracy 0.58\nstep 121, training accuracy 0.58\nstep 122, training accuracy 0.58\nstep 123, training accuracy 0.58\nstep 124, training accuracy 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 125, training accuracy 0.58\nstep 126, training accuracy 0.58\nstep 127, training accuracy 0.58\nstep 128, training accuracy 0.58\nstep 129, training accuracy 0.58\nstep 130, training accuracy 0.58\nstep 131, training accuracy 0.58\nstep 132, training accuracy 0.58\nstep 133, training accuracy 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 134, training accuracy 0.58\nstep 135, training accuracy 0.58\nstep 136, training accuracy 0.58\nstep 137, training accuracy 0.58\nstep 138, training accuracy 0.58\nstep 139, training accuracy 0.58\nstep 140, training accuracy 0.58\nstep 141, training accuracy 0.58\nstep 142, training accuracy 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 143, training accuracy 0.58\nstep 144, training accuracy 0.58\nstep 145, training accuracy 0.58\nstep 146, training accuracy 0.58\nstep 147, training accuracy 0.58\nstep 148, training accuracy 0.58\nstep 149, training accuracy 0.58\nstep 150, training accuracy 0.58\nstep 151, training accuracy 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 152, training accuracy 0.58\nstep 153, training accuracy 0.58\nstep 154, training accuracy 0.58\nstep 155, training accuracy 0.58\nstep 156, training accuracy 0.58\nstep 157, training accuracy 0.58\nstep 158, training accuracy 0.58\nstep 159, training accuracy 0.58\nstep 160, training accuracy 0.58\nstep 161, training accuracy 0.58"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nstep 162, training accuracy 0.58\nstep 163, training accuracy 0.58\nstep 164, training accuracy 0.58\nstep 165, training accuracy 0.58\nstep 166, training accuracy 0.58\nstep 167, training accuracy 0.58\nstep 168, training accuracy 0.58\nstep 169, training accuracy 0.58\nstep 170, training accuracy 0.58"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nstep 171, training accuracy 0.58\nstep 172, training accuracy 0.58\nstep 173, training accuracy 0.58\nstep 174, training accuracy 0.58\nstep 175, training accuracy 0.58\nstep 176, training accuracy 0.58\nstep 177, training accuracy 0.58\nstep 178, training accuracy 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 179, training accuracy 0.58\nstep 180, training accuracy 0.58\nstep 181, training accuracy 0.58\nstep 182, training accuracy 0.58\nstep 183, training accuracy 0.58\nstep 184, training accuracy 0.58\nstep 185, training accuracy 0.58\nstep 186, training accuracy 0.58\nstep 187, training accuracy 0.58\nstep 188, training accuracy 0.58"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nstep 189, training accuracy 0.58\nstep 190, training accuracy 0.58\nstep 191, training accuracy 0.58\nstep 192, training accuracy 0.58\nstep 193, training accuracy 0.58\nstep 194, training accuracy 0.58\nstep 195, training accuracy 0.58\nstep 196, training accuracy 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 197, training accuracy 0.58\nstep 198, training accuracy 0.58\nstep 199, training accuracy 0.58\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(200):\n",
    "    batch = get_next_batch(50, x_train, y_train_vec)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.625357\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: x_test, y_: y_test_vec, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}